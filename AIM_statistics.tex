%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Uppsala University Assignment Title Page 
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
% Modified by Elsa Slattegard to fit Uppsala university
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)

%\title{Title page with logo}
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{hyperref}

\tcbuselibrary{theorems}
\tcbuselibrary{skins}
\tcbuselibrary{breakable}
\usepackage{tikz}


\definecolor{blau}{HTML}{465555}
\definecolor{blau_dark}{HTML}{F75B4C}


\newtcolorbox{myframe}{
enhanced,
sharp corners=all,
colback=white,
colframe=blau,
toprule=0pt,
bottomrule=0pt,
leftrule=1.5pt,
rightrule=0pt,
overlay={\draw[blau,line width=1.5pt] (frame.south west) -- ++(1cm,0pt);}
}


\newtcbtheorem[
  auto counter,
  number within=section
]{definition}{Определение}{
  enhanced,
  breakable,                % ← IMPORTANT
  sharp corners=all,
  colback=white,
  colframe=blau,
  toprule=0pt,
  bottomrule=0pt,
  leftrule=1.5pt,
  rightrule=0pt,
  overlay={
    \draw[blau,line width=1.5pt]
      (frame.south west) -- ++(1cm,0pt);
  }
}{def}



\newtcbtheorem[
  auto counter,
  number within=section
]{theorem}{Теорема}{
  enhanced,
  breakable,
  sharp corners=all,
  colback=blau_dark!3,  % faint tint; adjust if needed
  colframe=blau_dark,
  toprule=0pt,
  bottomrule=0pt,
  leftrule=2pt,         % thicker emphasis
  rightrule=0pt,
  fonttitle=\bfseries,  % make “Theorem n.m” bold
  overlay={
    \draw[blau_dark,line width=2pt]
      (frame.south west) -- ++(1cm,0pt);
  }
}{thm}


\theoremstyle{definition}
\newtheorem{example}{Пример}

\theoremstyle{definition}
\newtheorem{problem}{Задача}

\theoremstyle{note}
\newtheorem{note}{Замечание}


\newcommand{\cat}[1]{\mathcal{#1}}
\newcommand{\fun}[1]{\mathfrak{#1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\IFish}{\mathcal{I}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\nr}[1]{\left\Vert #1 \right\Vert}
\newcommand{\bra}{\langle}
\newcommand{\ket}{\rangle}


\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE ИИИ МГУ}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.25]{aim_logo_trans_hr_black.png}\\[3cm] % Include a department/university logo - this will require the graphicx package
\textsc{\Large Конспект лекций}\\[0.5cm] % Major heading such as course name
\textsc{\large Антон Рябченко}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Математическая статистика и приложения}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Составитель конспекта:}\\
Вадим \textsc{Смирнов}\\ % Your name
\end{flushleft}

\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\tableofcontents

\section{Введение}
\subsection{Напоминание из теории вероятностей}
Этого напоминания не было в лекциях, но мне удобнее вставить его сюда.
\subsubsection{Основные определения}
\begin{definition}{Сигма-алгебра}{}
	$\sigma$-алгеброй на множестве $\Omega$ называется непустое семейство его подмножеств, замкнутое относительно дополнения до $\Omega$, не более чем счётных объединений и не более чем счётных пересечений.
\end{definition}
\begin{definition}{Измеримое множество}{}
	Множество с заданной на нём $\sigma$-алгеброй называется измеримым.
\end{definition}

\begin{definition}{Измеримое подмножество}{}
	Подмножество измеримого множества, принадлежащее его $\sigma$-алгебре, называется измеримым.
\end{definition}

\begin{definition}{Измеримая функция}{}
	Пусть $(\Omega_{1}, \sigma_{1})$, $(\Omega_{2}, \sigma_{2})$ - измеримые множества. Функция $f: \Omega_{1} \to \Omega_{2}$ называется измеримой, если полный прообраз любого измеримого подмножества в $\Omega_{2}$ является измеримым в $\Omega_{1}$, т.е.
	$$\forall s_{2} \in \sigma_{2}: \{ \omega_{1} \in \Omega_{1}| f(\omega_{1}) \in s_{2} \} \in \sigma_{1}$$
\end{definition}

\begin{note}
	Измеримые множества образуют категорию $\mathbf{Meas}$, морфизмами которой являются измеримые отображения. Это конкретная категория очевидным забывающим функтором $\square : \mathbf{Meas} \to \mathbf{Sets}$.
\end{note}

\begin{definition}{Вероятностная мера}{}
	Вероятностная мера - это функция $P$ из $\sigma$-алгебры множества $X$ в $[0, 1]\subset \mathbb{R}$, такая, что:
	\begin{enumerate}
		\item $P(\varnothing) = 0$
		\item $P(\bigcup_{k=1}^{\infty}s_{k})=\sum_{k=1}^{\infty}P(s_{k})$ для всякого не более чем счётного семейство попарно непересекающихся множеств $s_{k}\in \sigma$
		\item $P(\Omega) = 1$
	\end{enumerate}
\end{definition}

\begin{definition}{Вероятностное пространство}{}
	Тройка $(\Omega, \sigma, P)$, где $\Omega$ - множество, $\sigma$ - $\sigma$-алгебра, и $P$ - вероятностная мера, называется вероятностным пространством.
\end{definition}

\begin{definition}{Расширение вероятностного пространства}{}
	Говорят, что вероятностное пространство $(\Omega', \sigma', P')$ расширяет вероятностное пространство $(\Omega, \sigma, P)$, если задано сюръективное измеримое отображение $\pi: \Omega' \to \Omega$, сохраняющее вероятность, т.е. $\forall s \in \sigma : P'(\pi^{-1}(s))=P(s)$
\end{definition}

\begin{example}
	Вероятностное пространство бросков двух шестигранных кубиков расширяет вероятностное пространство бросков одного шестигранного кубика. Расширение задаётся сюръекцией $\pi: (x_{1}, x_{2})\to x_{1}$, где $x_{1}, x_{2}$ - исходы первого и второго броска соответственно. В качестве иллюстрации свойства сохранения вероятности рассмотрим событие в "выпадает единица или четвёрка" в вероятностном пространстве, соответствующем броску одного кубика. Его вероятность - 1/3. Полный праобраз этого события - "Выпадает единица или четвёрка на первом кубике, и что угодно на втором". Его вероятность - по-прежнему 1/3.
\end{example}

\begin{note}
	Вероятностные пространства также образуют категорию, морфизмами которой являются расширения вероятностных пространств.
\end{note}

\begin{definition}{Случайная величина}{}
	Случайной величиной называется измеримая функция $f$ из множества событий $\Omega$ вероятностного пространства $(\Omega, \sigma, P)$ в некоторое измеримое множество $(\cat{X}, \Sigma)$ с $\sigma$-алгеброй $\Sigma$.
\end{definition}

\begin{definition}{Реализация случайной величины}{}
	Реализациями случайной величины называются измеримые подмножества $X\subset \cat{X}$.
\end{definition}

\begin{definition}{Вероятность реализации}{}
	Вероятность реализации $X \subset \cat{X}$ - это $P(f^{-1}(X))$.
\end{definition}

Это определение корректно, так как $f$ измерима.

\begin{theorem}{Расширение сохраняет вероятности}{}
	Расширение вероятностных пространств сохраняет вероятность реализаций, т.е. 
	$$\forall X \subset \cat{X}: P'((f \circ \pi)^{-1}(X))=P(f^{-1}(X))$$
\end{theorem}
\begin{proof}
	Действительно, пусть $\pi: (\Omega', \sigma', P') \to (\Omega, \sigma, P)$ - расширение вероятностных пространств.
$$(f \circ \pi)^{-1}=\pi^{-1} \circ f^{-1}$$
$$P'(\pi^{-1}(f^{-1}(X)))=P(f^{-1}(X))$$
по определению расширения.
\end{proof}

\begin{definition}{Борелевское множество}{}
	Борелевским множеством называется подмножество топологического пространства, получаемое из открытых подмножеств этого пространства операциями дополнения, не более чем счётного пересечения и не более чем счётного объединения.
\end{definition}

\begin{theorem}{Алгебра борелевских множеств}{}
	Борелевские множества $\mathcal{B}(M)$ любого топологического пространства $M$ образуют $\sigma$-алгебру.
\end{theorem}
\begin{proof}
	Очевидно.
\end{proof}

\begin{example}
	$\mathbb{R}^{n}$ является топологическим пространством (с топологией, порождённой стандартной евклидовой метрикой, с базой из открытых шаров всевозможных ненулевых радиусов и со всевозможными центрами). Его борелевская сигма-алгебра порождается (при помощи операций, определяющих сигма-алгебру) этими же открытыми шарами.
\end{example}


Как правило, целевое множество является линейным пространством $(\mathbb{R}^n, +)$. Это позволяет определять интегральные характеристики случайных величин, например, математическое ожидание.

\begin{definition}{Функция случайной величины}{}
	Функцией $\mathcal{X}$-значной случайной величины $f: \Omega \to \mathcal{X}$, где $(\Omega, \sigma, P)$ - вероятностное пространство, называется измеримое отображение $g: \mathcal{X}\to \mathcal{Y}$ в измеримое пространство $\mathcal{Y}$. Функция такой случайной величины задаёт $\mathcal{Y}$-значную случайную величину $g\circ f$ на вероятностном пространстве $(\Omega, \sigma, P)$. Это действительно случайная величина, так как композиция измеримых отображений измерима.
\end{definition}

\begin{theorem}{Расширение - естественное преобразование}{}
	Расширение вероятностных пространств $\pi: \Omega' \to \Omega$ переводит функции случайных величин на $\Omega$ в функции случайных величин на $\Omega'$, сопоставляя случайной величине $f:\Omega \to \mathcal{X}$ случайную величину $f\circ \pi: \Omega' \to \mathcal{X}$. Вероятности всех реализаций $\mathcal{X}$ при этом сохраняются.
\end{theorem}
\begin{proof}
	Аналогично теореме о сохранении вероятности расширениями.
\end{proof}

\begin{definition}{Произведение сигма-алгебр}{}
	Пусть $(\Omega_{1}, \sigma_{1})$, $(\Omega_{2}, \sigma_{2})$ - измеримые пространства. Произведением $\sigma$-алгебр $\sigma_{1}$ и $\sigma_{2}$, обозначаемым $\sigma_{1}\otimes \sigma_{2}$, называется $\sigma$-алгебра на декартовом произведении $\Omega_{1}\times \Omega_{2}$, порождённая декартовым произведением $\sigma_{1}$ и $\sigma_{2}$ как наборов подмножеств $\Omega_{1}$ и $\Omega_{2}$, то есть минимальная по включению $\sigma$-алгебра на $\Omega_{1}\times \Omega_{2}$, содержащая $\sigma_{1}\times \sigma_{2}$ как подмножество.
\end{definition}

\begin{definition}{Распределение случайной велиины}{}
	Распределением случайной величины $f: \Omega \to \cat{X}$, заданной на вероятностном пространстве $(\Omega, \sigma, P)$ и принимающей значения в $(\cat{X}, \Sigma)$, называется вероятностная мера $p$, дополняющая пару $(\cat{X}, \Sigma)$ до вероятностного пространства, и определённая соотношением
	$$\forall S\in \Sigma: p(S):=P(f^{-1}(S))$$ 
\end{definition}

В дальнейшем мы будем обычно обозначать меру на исходном вероятностном пространстве и на пространстве значений одной и той же буквой $P$. Говорят, что случайная величина $f$ индуцирует меру $f^*P=p$ на $\mathcal{X}$, или что $p$ является пушфорвардом $P$ при помощи $f$.

\begin{definition}{Совместное распределение}{}
	Если две случайные величины $f_{1}, f_{2}$ заданы на одном и том же вероятностном пространстве $(\Omega, \sigma, P)$, и принимают значения в измеримых пространствах $(\cat{X}_{1}, \Sigma_{2})$, $(\cat{X}_{2}, \Sigma_{2})$, то определено их совместное распределение, то есть определена случайная величина $(f_{1}, f_{2})$ на вероятностном пространстве $(\Omega, \sigma, P)$, принимающая значения в измеримом пространстве $(\cat{X}_{1}\times\cat{X}_{2}, \Sigma_{1}\otimes\Sigma_{2})$, по правилу
	$$(f_{1}, f_{2})(\omega \in \Omega)=(f_{1}(\omega), f_{2}(\omega))$$
	$$P(X_{1}\times X_{2}\subset \cat{X}_{1}\times \cat{X}_{2})=P(f_{1}^{-1}(X_{1})\cap f_{2}^{-1}(X_{2}))$$
\end{definition}

\begin{note}\label{linearity_first}
	Если $(\Omega, \sigma, P)$ - вероятностное пространство,  $f_{1}:\Omega \to R_{1}, f_{2}:\Omega \to R_{2}$ - случайные величины, а $g: R_{1}\times R_{2}\to R$ измерима, то композиция $g \circ (f_{1}, f_{2})$ - случайная величина на $(\Omega, \sigma, P)$. Например, если $R_{1}, R_{2}, R=\mathbb{R}$, и $g(x_{1}, x_{2})=x_{1}+x_{2}$, таким образом определена сумма случайных величин $f_{1}+f_{2}$.
\end{note}

\begin{definition}{Функция распределения}{}
	Функцией распределения вещественнозначной случайной величины $f:\Omega \to \mathbb{R}$, где $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ - структура измеримого пространства на $\mathbb{R}$, называется функция $$F(x):=p((-\infty, x])$$  
\end{definition}

\begin{definition}{Плотность распределения}{}
	Плотностью распределения случайной величины $f:\Omega \to \mathbb{R}$ называется такая функция $\rho(x): \mathbb{R}\to  \mathbb{R}_{+}$, что для всякого $S\in \mathcal{B}(\mathbb{R})$ $$p(S)=\int_{S} \rho(x)dx$$
(если она существует).
\end{definition}

\begin{note}
	Если производная от $F(x)$ по $x$ существует, она равна $\rho(x)$ почти всюду относительно меры $p$ на области своего определения.
\end{note}

\subsubsection{Независимость случайных величин}
\begin{definition}{Независимость}{}
Две случайные величины $f_{1}:\Omega \to R_{1}, f_{2}:\Omega \to R_{2}$, определённые на одном и том же вероятностном пространстве $(\Omega, \sigma, P)$, со значениями в $(R_{1}, \Sigma_{1})$, $(R_{2}, \Sigma_{2})$ соответственно, называются независимыми, если для всякого измеримого подмножества $R_{1}\times R_{2}$ вида $X_{1}\times X_{2} \in \Sigma_{1}\times \Sigma_{2}$ выполняется
$$P(X_{1}\times X_{2})=P(X_{1})\cdot P(X_{2})$$
где $P(X_{1}\times X_{2})$ вычислено согласно совместному распределению $f_{1}$ и $f_{2}$
\end{definition}



\subsubsection{Моменты случайных величин и другие функционалы распределений}
\begin{definition}{Медиана}{}
Вещественное число $M(f)$ называется медианой случайной величины $f:\Omega \to \mathbb{R}$, если одновременно
$$P(f>M(f))\le 1/2$$
$$P(f<M(f))\le 1/2$$
то есть одновременно не более чем в половине случаев случайная величина принимает значения меньше медианы, и не более чем в половине случаев - больше медианы.
\end{definition}

\begin{theorem}{О существовании медианы}{}
Медиана случайной величины всегда существует. Если функциия распределения $f$, $F(x)$, непрерывна, то медиана также единственна.
\end{theorem}

\begin{definition}{Матожидание}{}
Если $\cat{X}$ - подмножество $\mathbb{R}^n$, $f:\Omega \to \cat{X}$ - случайная величина, $p=f^*P$ - индуцированная ей мера на $\cat{X}$, то лебегов интеграл 
$$\mathbb{E}[f]:= \int_{\cat{X}} x dp(x)$$
называется математическим ожиданием $f$. В случае, если плотность распределения $f$, $\rho(x)$, существует, то
$$\mathbb{E}[f]=\int_{\cat{X}}x\rho(x)dx$$
\end{definition}

\begin{note}\label{linearity_second}
	Если $f:\Omega \to \mathbb{R}^{n}$ - случайная величина, то $\forall c\in \mathbb{R}: cf: \Omega \to \mathbb{R}^n$ - также случайная величина.
\end{note}

\begin{theorem}{Линейность матожидания}{}
	Математическое ожидание линейно: если $f_{1}, f_{2}:\Omega \to \mathcal{X} \subseteq \mathbb{R}^n$ - случайные величины, то 
	$$\mathbb{E}[c_{1}f_{1}+c_{2}f_{2}]=c_{1}\mathbb{E}[f_{1}]+c_{2}\mathbb{E}[f_{2}]$$
\end{theorem}

\begin{proof}
Это просто линейность интеграла Лебега.
\end{proof}

\begin{theorem}{Монотонность матожидания}{}
	Математическое ожидание монотонно: если для пары случайных величин $f, g:\Omega \to \mathcal{X}\subseteq \mathbb{R}$ выполняется $\forall \omega\in \Omega: f(\omega)\le g(\omega)$, то $\mathbb{E}[f]\le \mathbb{E}[g]$. 
\end{theorem}
\begin{proof}
	Опять свойство интеграла Лебега.
\end{proof}

\begin{theorem}{Формула замены переменных}{}
	Если $f:\Omega \to \mathbb{R}$ - случайная величина, и $F:\mathbb{R}\to \mathbb{R}$ - измеримая абсолютно интегрируемая функция, то
	$$\mathbb{E}[F(f)]=\int_{\mathbb{R}}F(x)dp(x)=\int_{\mathbb{R}}F(x)\rho(x)dx$$
\end{theorem}
\begin{proof}

\end{proof}

\begin{theorem}{Матожидание произведения независимых случайных величин}{}
Матожидание произведения независимых случайных величин $f_{1}, f_{2}:\Omega\to \mathbb{R}$ равно произведению их матожиданий:
$$\mathbb{E}[f_{1}f_{2}]=\mathbb{E}[f_{1}]\mathbb{E}[f_{2}]$$
\end{theorem}
\begin{proof}
Следующее доказательство не очень строгое, зато понятное:
$$\mathbb{E}[f_{1}f_{2}]=\int_{\mathbb{R}\times \mathbb{R}}dp(x_{1}, x_{2})=\int_{\mathbb{R}\times \mathbb{R}}p(dx_{1}\times dx_{2})=$$
$$=\int_{\mathbb{R}\times \mathbb{R}}p(dx_{1})p(dx_{2})=\int_{\mathbb{R}}dp(x_{1})\int_{\mathbb{R}}dp(x_{2})=\mathbb{E}[f_{1}]\mathbb{E}[f_{2}]$$
\end{proof}


\begin{definition}{Моменты}{}
	$n$-ым моментом $\mathbb{R}$-значной случайной величины $f$, индуцирующей меру $p$ на $\cat{X}\subseteq \mathbb{R}$, называется интеграл
	$$M_{k}(f):=\mathbb{E}[f^n] = \int_{\cat{X}}x^{n}dp(x)=\int_{\cat{X}}x^n \rho(x)dx$$
	если указанный интеграл существует.
\end{definition}


\begin{definition}{Дисперсия}{}
	Дисперсией случайной величины $f:\Omega \to \mathbb{R}$ называется величина
	$$\mathbb{D}[f]:= \mathbb{E}[(f-\mathbb{E}[f])^2]$$
\end{definition}

\begin{theorem}{Дисперсия - кумулянт 2 порядка}{}
Константы выносятся из дисперсии с квадратом:
	$$\forall c\in \mathbb{R}: \mathbb{D}[cf]=c^2\mathbb{D}[f]$$
\end{theorem}
\begin{proof}
	$$\mathbb{E}[(cf-\mathbb{E}[cf])^2]=\mathbb{E}[c^2f^2-2\mathbb{E}[cf]cf+\mathbb{E}[cf]^2]=$$
	$$=c^2\mathbb{E}[f^2-2\mathbb{E}[f]f+\mathbb{E}[f]^2]=c^2\mathbb{D}[f]$$
\end{proof}

\begin{theorem}{Формула дисперсии}{}
	$$\mathbb{D}[f]=\mathbb{E}[f^2]-\mathbb{E}[f]^2$$
\end{theorem}
\begin{proof}
	Продолжая выкладку в предыдущем доказательстве и пользуясь линейностью матожидания,
	$$\mathbb{E}[f^2 -2\mathbb{E}[f]f+\mathbb{E}[f]^2]=\mathbb{E}[f^2]-2\mathbb{E}[f]^2+\mathbb{E}[f]^2$$ $$=\mathbb{E}[f^2]-\mathbb{E}[f]^2$$
\end{proof}

\begin{theorem}{Дисперсия суммы независимых случайных величин}{}
Если $f_{1}, f_{2}:\Omega \to \mathbb{R}$ - незавимисые случайные величины, то 
$$\mathbb{D}[f_{1}+f_{2}]=\mathbb{D}[f_{1}]+\mathbb{D}[f_{2}]$$
\end{theorem}
\begin{proof}
По формуле дисперсии

\begin{gather*}
\mathbb{D}[f_{1}+f_{2}]=\mathbb{E}[(f_{1}+f_{2})^2]-\mathbb{E}[f_{1}+f_{2}]^2 = \\
\mathbb{E}[f_{1}^2 + 2f_{1}f_{2}+f_{2}^2]-(\mathbb{E}[f_{1}]+\mathbb{E}[f_{2}])^2 = \\
\mathbb{E}[f_{1}^2] + \mathbb{E}[f_{2}^2] + 2\mathbb{E}[f_{1}f_{2}] - \mathbb{E}[f_{1}]^2 - \mathbb{E}[f_{2}]^2 - 2\mathbb{E}[f_{1}]\mathbb{E}[f_{2}]
\end{gather*}
Пользуясь свойством матожидания произведения
\begin{gather*}
=\mathbb{E}[f_{1}^2] + \mathbb{E}[f_{2}^2] + 2\mathbb{E}[f_{1}]\mathbb{E}[f_{2}] - \mathbb{E}[f_{1}]^2 - \mathbb{E}[f_{2}]^2 - 2\mathbb{E}[f_{1}]\mathbb{E}[f_{2}]=\\
=(\mathbb{E}[f_{1}^2]-\mathbb{E}[f_{1}]^2) + (\mathbb{E}[f_{2}^2]-\mathbb{E}[f_{2}]^2)=\\
\mathbb{D}[f_{1}]+\mathbb{D}[f_{2}]
\end{gather*}
\end{proof}

\begin{theorem}{Дисперсия инвариантна относительно сдвигов}{}
Для всякой случайной величины $f:\Omega \to \mathbb{R}$ и всякой константы (постоянной случайной величины) $c\in \mathbb{R}$
$$\mathbb{D}[f+c]=\mathbb{D}[f]$$
\end{theorem}
\begin{proof}
$$\mathbb{D}[f+c]=\mathbb{E}[(f+c)^2]-\mathbb{E}[f+c]^2=$$
$$=\mathbb{E}[f^2]+2\mathbb{E}[fc]+\mathbb{E}[c^2]-\mathbb{E}[f]^2-2\mathbb{E}[c]\mathbb{E}[f]-\mathbb{E}[c]^2$$
$$=\mathbb{E}[f^2]+2c\mathbb{E}[f]+c^2-\mathbb{E}[f]^2-2c\mathbb{E}[f]-c^2$$
$$=\mathbb{E}[f^2]-\mathbb{E}[f]^2=\mathbb{D}[f]$$
\end{proof}

\begin{definition}{Стандартное отклонение}{}
	Стандартным отклонением $\sigma$ случайной величины $f$, имеющей дисперсию $\mathbb{D}[f]$, называется её положительный корень $\sqrt{\mathbb{D}[f]}$
\end{definition}

Из свойств дисперсии следует, что стандартное отклонение неотрицательно и линейно для независимых случайных величин.


\begin{definition}{Факториальная производящая функция}{}
	Факториальной производящей функцией моментов случайной величины $f:\Omega \to \mathbb{R}$ называется функция
	$$M(t)=\mathbb{E}\left[\frac{1}{1-tf}\right]=\sum_{k=0}^{\infty}t^{k}M_{k}(f)$$
\end{definition}

\begin{definition}{Производящая функция моментов}{}
	Производящей функцией моментов случайной величины $f:\Omega \to \mathbb{R}$ называется функция
	$$G(t)=\mathbb{E}[e^{tf}]=\sum_{k=0}^{\infty}\frac{t^k}{k!}M_{k}(f)$$
\end{definition}

\begin{definition}{Производящая функция кумулянтов}{}
	Производящей функцией кумулянтов случайной величины $f:\Omega \to \mathbb{R}$ называется функция
	$$\Gamma(t) := \ln G(t) = \sum_{k=1}^{\infty} \frac{t^k}{k!} K_{k}(f)$$ где величина $K_{k}(f)$ называется $k$-ым кумулянтом $f$.
\end{definition}

\begin{example}
	$K_{1}(f)=\mathbb{E}[f]$, $K_{2}(f)=\mathbb{D}[f]$
\end{example}

\begin{note}
	Кумулянты иногда называют полунинвариантами. На языке комбинаторики факториальная производящая функция моментов - это обыкновенная производящая функция, производящая функция - экспоненциальная производящая функция. Физики знают производящую функцию кумулянтов как производящий функционал связных диаграмм Фейнмана; в общем случае в комбинаторике кумулянты соответствуют связным, в некотором смысле, объектам.
\end{note}

\begin{definition}{Характеристическая функция}{}
	Характеристической функцией случайной величины $X:\Omega \to \mathbb{R}$ называется <<преобразование Фурье>> её плотности:
	\begin{equation}
		\chi(t):=\mathbb{E}[\exp(itX)]
	\end{equation}
\end{definition}

\subsubsection{Вероятностные неравенства}
\begin{theorem}{Неравенство Маркова}{}
\label{thm:markov_ineq}
	Если случайная величина $f:\Omega \to \mathbb{R}$ принимает только неотрицательные значения, то
	$$\forall \lambda \ge 0: P(f\ge \lambda) \le \frac{1}{\lambda}\mathbb{E}[f]$$
\end{theorem}
\begin{proof}
	Рассмотрим случайную величину $\lambda \mathbb{I}(f\ge \lambda)(\omega)$, которая равна нулю на тех $\omega\in \Omega$, на которых $f(\omega) < \lambda$, и $\lambda$ на тех $\omega$, на которых $f(\omega)\ge \lambda$.  В силу линейности математического ожидания
	$$\mathbb{E}[\lambda \mathbb{I}(f\ge \lambda)]=\lambda \mathbb{E}[\mathbb{I}(f\ge \lambda)]=\lambda P(f\ge \lambda)$$
В силу монотонности математического ожидания,
$$\lambda P(f\ge \lambda) \le \mathbb{E}[f]$$
\end{proof}
\begin{theorem}{Экспоненциальная граница}{}
Если случайная величина имеет $k$-ый абсолютный момент, то есть $\E [[|f|^k] \le C \in \mathbb{R}$ то 
$$\forall \lambda > 0: P(|f|\ge \lambda) \le C\lambda^{-k}$$
\end{theorem}
\begin{proof}
Из неравенства Маркова
$$P(|f| \le \lambda) = P(|f|^k \le \lambda^k) \le \frac{1}{\lambda^k}\mathbb{E}[|f|^k]$$
\end{proof}

\begin{theorem}{Неравенство Чебышёва}{}
Пусть $\hat{\sigma}(f):= |f-\mathbb{E}[f]|$ - модуль отклонения случайной величины от её среднего. Тогда
$$P(\hat{\sigma}(f)\ge \lambda) \le \frac{\mathbb{D}[f]}{\lambda^2}$$
\end{theorem}
\begin{proof}
Это неравенство Маркова для второго абсолютного момента $\hat{\sigma}(f)$.
\end{proof}

\begin{theorem}{Неравенство Йенсена}{}
	Пусть $f:\mathbb{R}\to \mathbb{R}$ - выпуклая функция, и пусть $X$ - ограниченная вещественнозначная случайная величина (т.е. вещественнозначная случайная величина с ограниченным носителем). Тогда
	\begin{equation}
		\E[f(X)]\ge f(\E[X])
	\end{equation}
Если же функция $f$ вогнутая, то при тех же условиях
	\begin{equation}
		\E[f(X)] \le f(\E[X])
	\end{equation}
\end{theorem}



\subsubsection{Линейная и евклидова структура пространств случайных величин}
\paragraph{Два способа складывать случайные величины}\

На множествах случайных величин существует две \textit{разные} структуры вещественного векторного пространства. Говоря более точно, разные структуры имеют множество случайных величин на заданном пространстве и множество всех случайных величин, полученных из данного вероятностного пространств. Чтобы понять разницу, заметим, что в использованном нами выше смысле сумма двух одинаково распределённых случайных величин, заданных на одном и том же вероятностном пространстве, не равна ни одной из них, взятой двухкратно. Скажем, рассмотрим бернуллиевскую случайную величину $\xi_{p}$, принимающую значения $0$ и $1$ с вероятностью $p$ и $1-p$. Её можно рассматривать как тождественную функцию из множества $\{0, 1\}$ в $\{0, 1\}\subset \mathbb{R}$. Используемая нами сигма-алгебра - $\{ \varnothing, \{0\}, \{1\}, \{0, 1\} \}$, а вероятностная мера - функция, заданная таблицей
$$P(\varnothing)=0, \ P(\{1\})=p, \ P(\{0\})=1-p, \ P(\{0, 1\})=1$$

Мы можем сложить поточечно две одинаковые случайные величины $\xi_{p}$ с коэффициентами $\alpha$ и $\beta$ как измеримые функции $\{0,  1\}\to \mathbb{R}$, и получить новую измеримую функцию $\{0, 1\}\to \mathbb{R}$:
$$\alpha \xi_{p}+\beta \xi_{p}:\{0, 1\} \to \{0, \alpha + \beta\}=(\alpha + \beta)\xi_{p}$$
Её математическое ожидание равно, очевидно, $(\alpha+\beta)p$.\\

Аналогично, произвольная измеримая функция из $\{0, 1\}$ в $\mathbb{R}$ имеет вид
$$f_{\alpha, \beta}:\{0, 1\}\to \{\alpha, \beta\}$$
Она может принимать не более двух значений, и нетрудно видеть, что такие случайные величины образуют двумерное вещественное линейное пространство с базисом $\{f_{1,0}, f_{0, 1}\}$. Математическое ожидание для случайной величины $f_{\alpha, \beta}$ равно, очевидно, $(1-p)\alpha + p\beta$. При этом случайная величина $f_{0, 0}:\{0, 1\}\to \{0, 0\}$, всегда принимающая значение 0, является нулём этого линейного пространства.\\

Сложим две теперь две \textit{независимые} случайные величины $\xi_{p1}$ и $\xi_{p2}$, имеющие одинаковое распределение, с коэффициентами $\alpha$ и $\beta$. Для этого нам сначала нужно определить их совместное распределение. Заметим, что независимые одинаково распределённые случайные величины не могут быть совпадать как измеримые функции. Действительно, чтобы сложить две случайные величины, нужно сначала определить их совместное распределение. Мы конструируем случайную величину $(\xi_{p1}, \xi_{p2})$ по правилу
$$(\xi_{p1}, \xi_{p2})(\omega \in \Omega):= (\xi_{p1}(\omega), \xi_{p2}(\omega))$$
т.е. $(\xi_{p1}, \xi_{p2}):\{0, 1\}\to (\{0, 0\}, \{1, 1\})$. Вычислим $P(\{0, 1\})$:
$$P(\{0, 1\})=P(\xi_{1p}^{-1}(\{0\}) \cap \xi_{2p}^{-1}(\{1\}))=P(\{0\}\cap \{1\})=0$$
Поскольку для независимых случайных величин $$P_{(\xi_{1}, \xi_{2})}(A\cap B)=P_{\xi_{1}}(A)P_{\xi_{2}}(B)$$ и ни один из сомножителей не равен нулю, приходим к противоречию. Отсюда вывод: сложение одинаково распределённых случайных величин требует расширения вероятностного пространства, на котором они определены. Итак, вместо пространства $(\{0, 1\}, 2^{\{0, 1\}}, P)$ мы используем пространство $(\{0, 1\}^{\times 2}, 2^{\{0, 1\}^{\times 2}}, P \times P)$, и определяем на нём величину $(\xi_{p1}, \xi_{p2})$ как 
$$\xi_{p1}\times \xi_{p2}((\omega_{1}, \omega_{2}))=(\xi_{p1}(\omega_{1}), \xi_{p2}(\omega_{p2}))$$
Теперь мы можем сложить их как $\alpha\xi_{p1}+\beta\xi_{2}$ (умножение на скаляр определено на каждом члене прямого произведения так же, как в первом случае), и получить новую случайную величину на том же вероятностном пространстве,
$$\alpha \xi_{p1}+ \beta \xi_{p2}:(\omega_{1}, \omega_{2})\to \alpha \xi_{p1}(\omega_{1}) + \beta \xi_{p}(\omega_{2})$$
Эта случайная величина может принимать уже четыре значения - $\{0, \alpha, \beta, \alpha + \beta\}$ - с вероятностями $\{(1-p)^2, p(1-p), p(1-p), p^2\}$ соответственно. При этом, в силу линейности матожидания, оно у неё также будет равно $p(\alpha + \beta)$, что проверяется прямым вычислением:
$$(\alpha + \beta)(p(1-p)+p^2)=(\alpha + \beta)p(p+1-p)=(\alpha + \beta)p$$

Итак, существует две линейные структуры, связанные со множествами случайных величин. Разберёмся, как работает каждая из них.



\paragraph{Линейное случайных величин на одном вероятностном пространстве}\
\begin{theorem}{Структура линейного пространства распределений случайных величин}{}
	Пусть задано вероятностное пространство $(\Omega, \sigma, P)$. Тогда случайные величины, являющиеся измеримыми функциями из $\Omega$ в $\mathbb{R}^n$, образуют вещественное линейное пространство относительно операций поточечного сложения и умножения на число.
	
\end{theorem}
\begin{note}
	Вообще, более правильно говорить о структуре линейного пространства на классах эквивалентности измеримых функций - две функции называются эквивалентными, если различаются на множестве меры 0 (т.е. $P(\xi_{1}\ne \xi_{2})=0$, т.е. две величины совпадают с вероятностью 1).	
\end{note}
\begin{proof}
	Сумма и линейная комбинация двух классов эквивалентности измеримых функций измерима. Класс эквивалентности случайной величины, имеющей плотность распределения $\delta(x)$ (то есть целиком сконцентрированной в нуле) является нейтральным элементом относительно сложения.
\end{proof}

 Это пространство естественным образом является нормированным. 

\begin{definition}{$L_{p}$-нормы на пространстве случайных величин}{}
Пусть $p\in[1, \infty)$. Пусть $X$ - случайная величина на пространстве $(\Omega, \sigma, P)$. Тогда 
\begin{equation}
	\nr{X}_{p}:=(\E\left[|X|^p\right])^{1/p}
\end{equation}
\end{definition}

Нетрудно проверить, что указанные фукнционалы действительно являются нормами, а именно:

\begin{enumerate}
	\item $\nr{X}_{p}\ge 0$, и $\nr{X}_{p}=0 \iff P(X=0) = 1$
	\item $\forall \alpha \in \mathbb{R}: \nr{\alpha X}_{p}=|\alpha|\nr{X}_{p}$
	\item $\forall X_{1}, X_{2}: \nr{X_{1}+X_{2}}_{p}\le \nr{X_{1}}_{p} + \nr{X_{2}}_{p}$ (неравенство треугольника)
\end{enumerate}


Действительно, первое и второе свойство очевидны и являются следствием линейности интеграла Лебега, а третье является следствием неравенства Минковского. Альтернативный способ\cite{random_services} доказательства неравенства треугольника состоит в использовании неравенства Йенсена: заметим, что $\mathbb{R}_{+}\times \mathbb{R}_{+}$ - выпуклое множество, а функция
$$g(x, y)=(x^{1/p}+y^{1/p})^{p}$$
вогнута на этом множестве.

 Тогда по неравенству Йенсена для неотрицательных случайных величин $U, V$ 
$$\E[(U^{1/p}+V^{1/p})^p]\le (\E[U]^{1/p}+\E[V]^{1/p})^p$$
Подставляя в качестве $U, V$ $|X|^{p}$, $|Y|^{p}$ соответственно, получаем
$$\E[(|X|+|Y|)^{p}]\le ( \E[|X|^{p}]^{1/p}+\E[|Y|^{p}]^{1/p} )^{p}$$
Извлекая корень степени $p$ из обеих частей и используя неарвенство $|X|+|Y|\ge |X+Y|$, а также монотонность матожидания, получаем требуемый результат.\\

В силу неравенства треугольника сумма двух случайных величин конечной $L_{p}$-нормы является случайной величиной конечной $L_{p}$-нормы, а потому такие случайные величины, заданные на данном вероятностном пространстве, образуют линейное пространство ${L}_{p}$, являющееся знакомым из функционального анализа лебеговым функциональным пространством.\\

\begin{theorem}{Неравенство Ляпунова}{}
	Пусть $1\le j \le k$. Пусть $X$ - случайная величина. Тогда
	$$\nr{X}_{j}\le \nr{X}_{k}$$
\end{theorem}
\begin{proof}
	Рассмотрим функцию $x^{j/k}$. Она выпукла и неотрицательна на $\mathbb{R}_{+}$. Применим неравенство Йенсена к $x=|X|^k$ и получим требуемое.
\end{proof}


Как известно из функционального анализа, любое нормированное пространство является метризуемым: метрика задаётся при помощи выражения
\begin{equation}
	\rho_{p}(X, Y)=\nr{X-Y}_{p}
\end{equation}
Таким образом, в частности, стандартное отклонение оказывается $L_{2}$-расстоянием от случайной величины до вырожденной случайной величины, равной её матожиданию на множестве меры 1:
$$\sigma(X)=\nr{X-\E[X]}_{2}=(\E[(X-\E[X])^2])^{1/2}$$

\begin{definition}{RMSE}{}
	Среднеквадратичным отклонением (ошибкой) вещественнозначной случайной величины $X$ от константы $t$ (root mean squared error) называется функция
	\begin{equation}
		\rho_{2}(X, t)=\nr{X-t}_{2}
	\end{equation}
\end{definition}

\begin{theorem}{Минимум среднеквадратичной ошибки достигается на среднем}{}
	Пусть $X$ - вещественнозначная случайная величина. Тогда 
	$$\arg\min_{t}\rho_{2}(X, t)=\E[X]$$
и 
	$$\min_{t}\rho_{2}(X, t)=\sigma(X)$$
\end{theorem}
\begin{proof}
	\begin{align*}
				\E[(X-t)^2]=\E[X^2 - 2tX + t^2]=& \\
	\E[X^2] - 2t\E[X] + t^2
	\end{align*}
Дифференцируя по $t$, получаем, что экстремум достигается в точке
    $$2(t-\E[X])=0$$
и очевидным образом является минимумом. Таким образом, при минимальном значении RMSE 
$$\rho_{2}(X, t)^2=\E[X^2]-\E[X]^2=\D[X]$$
и $\rho_{2}(X, t)=\sigma(X)$.
\end{proof}

\begin{definition}{Среднее абсолютное отклонение}{}
	Средним абсолютным отклонением вещественнозначной случайной величины $X$ от величины $t$ называется
	$$\rho_{1}(X, t):=\nr{X-t}_{1}$$ 
\end{definition}
\begin{theorem}{Медиана минимизирует среднее абсолютное отклонение}{}
Если $X$ - вещественнозначная случайная величина, то
	$$\arg\min_{t}\rho_{1}(X, t)=M(X)$$
\end{theorem}
Поскольку медиана может не быть единственной, среднее абсолютное отклонение может иметь больше одного минимума.


\paragraph{Евклидова структура пространства случайных величин}\

	Пространство $L_{2}$, как известно из функционального анализа, является не только банаховым, но и гильбертовым (т.е. банаховым и евклидовым). В вероятностных терминах скалярное произведение на $L_{2}$ определяется следующим образом:

\begin{definition}{Скалярное произведение на $L_{2}$ $\mathbb{R}$-значных случайных величин}{}
	Для вещественнозначных случайных величин $X, Y$, заданных на вероятностном пространстве $(\Omega, \sigma, P)$ и принадлежащих $L_{2}$, скалярное произведение определено как
	 \begin{equation}
	 	\langle X, Y\rangle := \E[XY]
	 \end{equation}
\end{definition}

Это действительно скалярное произведение, т.к. функция $\bra \cdot, \cdot \ket$ билинейна, симметрична и невырождена.

Случайные величины из $L_{2}$, имеющие нулевой первый момент, очевидным образом образуют подпространство в $L_{2}$ (например, потому, что это ядро линейного отображения матожидания). Любую случайную величину $X$ из $L_{2}$ можно \textbf{центрировать}, отняв от неё её матожидание, и превратив, таким образом, в случайную величину с нулевым матожиданием. Обозначим эту операцию $$C(X):=X-\E[X]$$ Отображение центрирования линейно в силу линейности матожидания. При этом отображение центрирования не изменяет центрированную случайную величину. Значит, $C^2=C$, и $C$ является проектором на подпространство центрированных случайных величин. Из линейной алгебры известно, что $L_{2}$, в таком случае, разлагается на прямую сумму
$$L_{2}=\text{Ker }C \oplus \text{Im }C$$
При этом $\text{Ker }C$ - это одномерное подпространство случайных величин с дельта-распределением $\rho_{X}(x)=\delta(x-\E[X])$. Таким образом, любую случайную величину из $L_{2}$ можно однозначно представить в виде 
$$X=C(X)+\E[X]$$
Тогда
$$\bra X, Y\ket =\bra C(X)+\E[X], C(Y)+\E[Y]\ket$$
$$=\bra C(X), C(Y)\ket + \E[X]\bra 1, C(Y)\ket + \E[Y]\bra 1, C(X)\ket + \E[X]\E[Y]\bra 1, 1\ket$$
$$=\bra C(X), C(Y)\ket + \E[X]\E[C(Y)]+\E[Y]\E[C(X)]+\E[X]\E[Y]$$
$$=\bra C(X), C(Y)\ket + \E[X]\E[Y]$$
Таким образом, $L_{2}$ оказывается не просто прямой суммой, а ортогональным разложением на подпространства констант и случайных величин с нулевым матожиданием. Можно рассматривать вырожденное <<скалярное произведение>>, взяв только его компоненту на пространстве центрированных случайных величин. Такое скалярное произведение называется \textbf{ковариацией}.

\begin{definition}{Ковариация}{}
	Ковариация случайных величин $X, Y$, принадлежащих $L_{2}$, определяется как
	\begin{equation}
		\textsc{cov}(X, Y):=\bra X-\E[X], Y-\E[Y]\ket=\bra C(X), C(Y)\ket
	\end{equation}
\end{definition}

Из билинейности скалярного произведения следует также, что

\begin{equation}
	\textsc{cov}(X, Y)=\E[XY]-\E[X]\E[Y]
\end{equation}
и, в частности,
\begin{equation}
	\textsc{cov}(X, X)=\D[X]
\end{equation}


Из этой формулы следует, что если пара случайных величин $X, Y$ независима, то $\textsc{cov}(X, Y)=0$. Действительно, в таком случае $\E[XY]=\E[X]\E[Y]$.\\


Если в пространстве задано скалярное произведение, то определён (при условии выполнения неравенства Коши-Буняковского-Шварца) угол между двумя векторами. Обычно определяется угол между центрированными случайными величинами. Косинус этого угла называется их корреляцией:

\begin{definition}{Корреляция случайных величин}{}
Корреляцией вещественнозначных случайных величин $X, Y\in L_{2}$ называется величина
	\begin{equation}
		\textsc{cor}(X, Y):= \frac{\bra C(X), C(Y)\ket}{\sqrt{\D[X]\D[Y]}}
	\end{equation}
\end{definition}

Выполнение неравенства Коши-Буняковского-Шварца для $L_{2}$-нормы и определённого выше скалярного произведения следует из того факта, что по определению
\begin{equation}
	\sqrt{\bra X, X\ket} := \nr{X}_{2}
\end{equation}
Таким образом, как известно из линейной алгебры, верна 
\begin{theorem}{Неравенство КБШ}{}
Для случайных величин $X, Y\in L_{2}$ выполняется
	\begin{equation}
		|\bra X, Y\ket|\le \nr{X}_{2}\nr{Y}_{2}
	\end{equation}
\end{theorem}
Из неравенства КБШ следует неравенство для ковариации
\begin{equation}
	|\textsc{cov}(X, Y)|\le \sigma(X)\sigma(Y)
\end{equation} 
и корреляции:
\begin{equation}
	-1 \le \textsc{cor}(X, Y)\le 1
\end{equation}

Очевидным следствием свойств скалярного произведения является

\begin{theorem}{Теорема Пифагора (равенство Парсеваля)}{}
	Пусть $\{X_{i}\}_{i=1}^{n}$ - набор попарно ортогональных случайных величин из $L_{2}$. Тогда
	$$\nr{\sum_{i=1}^n X_{i}}^2_{2}=\sum_{i=1}^{n}\nr{X_{i}}_{2}^2$$
\end{theorem}

Структура скалярного произведения также позволяет определить проекции случайных величин на подпространства. Из линейной алгебры известно, что проекция вектора $x$ на подпространство евклидова пространства, порождённое вектором $y$, даётся выражением
\begin{equation}
	\pi_{y}(x) = \frac{\bra x, y\ket}{\bra y, y\ket}y
\end{equation}
Аналогично можно определить проекцию случайной величины $X$ на подпространство, порождённое случайной величиной $Y$:
\begin{equation}
	\pi_{Y}(X):= \frac{\bra X, Y\ket}{\D[Y]}Y
\end{equation}

Проекция обладает тем ствойством, что это наиболее близкий к $X$ по $L_{2}$-норме вектор в линейной оболочке $Y$. Аналогично и с сохранением этого свойства, можно спроецировать случайную величину на произвольное подпространство случайных величин. 
\begin{definition}{Матрица ковариации}{}
	Матрица Грама $\bra C(X_{i}), C(X_{j})\ket$ набора центрированных случайных величин $\{X_{i}\}_{i=1}^n$ называется \textbf{матрицей ковариации} этого набора. 
\end{definition}



\paragraph{Алгебра случайных величин}\

Определим теперь алгебру случайных величин со сложением во втором смысле. 

\begin{definition}{Алгебра случайных величин}{}
Пусть $\mathcal{A}=\{a_{i}\}$ - алфавит формальных символов. Пусть для каждого формального символа задано вероятностное пространство $(\Omega_{a_{i}}, \sigma_{a_{i}}, P_{a_{i}})$. Рассмотрим свободную коммутативную алгебру, порождённую $\mathcal{A}$.
\end{definition}



\begin{theorem}{Структура алгебры случайных величин}{}
	Множество всех случайных величин на данном вероятностном пространстве образует линейное пространство (и, более того, бесконечно-порождённую ассоциативную алгебру) с операцией сложения, определённой следующим образом: если $f_{1}:\Omega \to \mathbb{R}_{1}^{n}$, $f_{2}:\Omega \to \mathbb{R}_{2}^{n}$, то $f_{1} + f_{2}: \Omega \to \mathbb{R}_{3}^{n}$ определено 
	\begin{equation}
		f_{1} + f_{2} := + \circ (f_{1}, f_{2})
	\end{equation}
\end{theorem}



\subsubsection{Типы сходимости}
Наличие метрики на пространстве распределений позволяет определить сходимость. Существует несколько типов сходимости случайных величин, связанных друг с другом нетривиальным образом.

\paragraph{Сходимость в среднем}\

$L_{p}$-метрики задают на пространстве распределений $L_{p}$ сходимость в $p$-среднем, которая определяется стандартным для нормированных пространств образом: сходящимися называются последовательности, удовлетворяющие критерию Коши из матанализа.

\begin{definition}{Сходимость в $p$-среднем}{}
Последовательность случайных величин $\{X_{n}\}_{n=1}^{\infty}$, заданных на вероятностном пространстве $(\Omega, \sigma, P)$, снабжённом $L_{p}$-нормой $\nr{\cdot}_{p}$, называется сходящейся, если
\begin{equation}
	\forall \varepsilon > 0: \exists N \in \mathbb{N}: \forall n,m > N: \nr{X_{n}-X_{m}}_{p} < \varepsilon
\end{equation}
\end{definition}

\begin{definition}{Сходимость к случайной величине в $p$-среднем}{}
	Говорят, что последовательность случайных величин $\{X_{n}\}_{n=1}^{\infty}$ сходится к случайной величине $X$ в $p$-среднем, если
\begin{equation}
	\lim_{n\to \infty}\rho_{p}(X_{n}, X) = 0
\end{equation}
Сходимость по $L_{1}$-норме называется \textbf{сходимостью в среднем}, сходимость по $L_{2}$-норме - \textbf{сходимостью в среднеквадратичном}.

\end{definition}

Из неравенства Ляпунова следует, что сходимость в $L_{p}$ влечёт сходимость в $L_{q}$ при $q\le p$. Больше того, из функционального анализа известно, что $L_{p}$ - банаховы пространства, а значит, любая сходящаяся в $L_{p}$-норме случайная величина имеет предел с конечной $L_{p}$-нормой.




\paragraph{Сходимость почти наверное}\

\begin{definition}{Сходимость почти наверное}{}
Последовательность случайных величин $X_{n}$ называется \textbf{сходящейся почти наверное} к случайной величине $X$, если 
\begin{equation}
	P\left(\lim_{n\to \infty}X_{n}=X\right) = 1
\end{equation}
\end{definition}
Сходимость почти наверное обозначается
$$X_{n}\stackrel{\text{a.s.}}{\rightarrow}X$$
(от английского almost surely).

\paragraph{Сходимость по вероятности}\

\begin{definition}{Сходимость по вероятности (по мере)}{}
	Пусть $f$ - измеримая функция на пространстве с мерой $(X, \Sigma, \mu)$, $\{f_{n}\}_{n=1}^{\infty}$ - последовательность таких функций. Говорят, что последовательность $\{f_{n}\}_{n=1}^{\infty}$ сходится к $f$ по вероятности, если
	\begin{equation}
		\lim_{n\to \infty} \mu(\{ x\in X: |f_{n}-f|\ge \varepsilon \}) = 0
	\end{equation}
\end{definition}


\begin{theorem}{Сходимость почти наверное сильнее сходимости по вероятности}{}
	...но в законе больших чисел не участвует.
	Говоря более точно, сходимость почти наверное влечёт сходимость  по вероятности, однако обратное в общем случае неверно.
\end{theorem}

\begin{theorem}{Сходимость в среднем сильнее сходимости по вероятности}{}
	Пусть $\{X_{n}\}_{n=1}^{\infty}$ сходится в среднем к $X$. Тогда $\{X_{n}\}_{n=1}^{\infty}$ сходится по вероятности к $X$, однако обратное в общем случае неверно.
\end{theorem}
\begin{proof}
	Используем неравенство Маркова~(\ref{thm:markov_ineq}). Пусть $\varepsilon > 0$. Тогда
$$0\le P(|X_{n}-X|>\varepsilon) \le \E[|X_{n}-X|]/\varepsilon \to 0$$
при стремлении $n$ к бесконечности.
\end{proof}


\paragraph{Сходимость по распределению}\
\begin{definition}{Сходимость по распределению}{}
	Пусть $(\Omega, \sigma, P)$ - вероятностное пространство. Пусть $\{X_{n}\}_{n=1}^{\infty}$ - последовательность случайных величин $X_{n}:\Omega \to \mathbb{R}^m$. Говорят, что последовательность $\{X_{n}\}_{n=1}^{\infty}$ сходится по распределению к случайной величине $X$, если для всякого открытого множества $A\subseteq \mathbb{R}^n$
\begin{equation}
	\lim_{n\to \infty}P(X_{n}\in A) = P(X\in A)
\end{equation}
\end{definition}

Сходимость по распределению - самый слабый тип сходимости из используемых нами. Из неё не следуют никакие другие сходимости, а она, в свою очередь, следует из остальных. Иными словами, верна следующая

\begin{theorem}{Сходимость по вероятности сильнее сходимости по распределению}{}
	Сходимость по вероятности влечёт за собой сходимость по распределению.
\end{theorem}


Таким образом, верны следующие взаимоотношения между разными типами сходимости:

\begin{itemize}
	\item Сходимость в $p$-среднем и сходимость почти наверное - самые сильные типы сходимости. Сходимость в $p$-среднем влечёт сходимость в $q$-среднем при $p\ge q$, но ни для какого $p$ сходимость в $p$-среднем не влечёт сходимость почти наверное, и наоборот.
	\item Сходимость по вероятности слабее и сходимости в среднем, и сходимости почти наверное. Для равномерно интегрируемых распределений сходимость по вероятности эквивалентна сходимости в среднем.
	\item Сходимость по распределению слабее сходимости по вероятности. 
\end{itemize}




\begin{thebibliography}{9}
\bibitem{tao}
Terrence Tao,
\textit{Topics in Random Matrix Theory}.
AMS, 2012, глава 1.

\bibitem{rota_comb}
Joseph P.S. Kung, Gian-Carlo Rota, Catherine H. Yan,
\textit{Combinatorics: The Rota Way}.
Cambridge University Press, 2009, глава 4.

\bibitem{rota_taylor_1994}
G.-C. Rota, B.D. Taylor, 
\textit{The Classical Umbral Calculus}.
SIAM J. Math. Anal. Vol. 25, No. 2, pp. 694-711, March 1994.

\bibitem{random_services}
Kyle Siegrist,
Random: Probability, Mathematical Statistics, Stochastic Processes,
https://www.randomservices.org/random/index.html

\end{thebibliography}


\section{Оценивание}
\subsection{Лекция 1}


\subsubsection{Основные определения}
\begin{definition}{Выборка из распределения}{}
	Выборкой $X$ размера $n$ из распределения $\mu$ называется набор независимых случайных величин $\{X_{i}\}_{i=1}^{n}$ со значениями в $\cat{X}$, каждая из которых имеет распределение $\mu$.
\end{definition}

Выборкой называют также конкретную реализацию такого набора случайных величин. Различие обычно ясно из контекста.

\begin{definition}{Статистика}{}
	Семейство $\mu$-измеримых функций $f: \cat{X}^{\times n} \to \mathbb{R}$ такое, что для каждого натурального $n$ в семействе есть одна и только одна функция, называется статистикой.
\end{definition}

Будем обозначать $\mathcal{P}$ семейство распределений $\{\mu_{\theta}\}_{\theta \in \Theta}$, где $\Theta$ - некоторое множество.

\begin{definition}{Генеральная совокупность}{}
	Генеральной совокупностью называется множество всех  объектов, о которых мы делаем некоторые статистические выводы.
\end{definition}

Генеральная совокупность - не столько математическое, сколько <<физическое>> понятие. Обычно мы моделируем генеральную совокупность при помощи некоторого распределения или семейства распределений. Например, мы хотим оценить усреднённые характеристики всех потенциальных клиентов компании. Множество всех потенциальных клиентов - генеральная совокупность. Генеральная совокупность, как правило, недоступна для наблюдения целиком (иначе все интересующие нас характеристики можно было бы вычислить в явном виде, не прибегая к статистике). Поэтому мы пытаемся моделировать множество всех потенциальных клиентов при помощи некоторого семейства распределений на множестве возможных характеристик потенциальных клиентов. 

\begin{definition}{Выборка из генеральной совокупности}{}
Выборкой из генеральной совокупности называется любое подмножество генеральной совокупности. Альтернативно, выборкой называется случайная величина со значениями во множестве подмножеств генеральной совокупности. 
\end{definition}

Иными словами, выборку из генеральной совокупности можно рассматривать как результат случайного процесса, выбирающего некоторые элементы из генеральной совокупности.

\begin{definition}{Эмпирическая функция распределения}{}
Пусть $\{X_{i}\}_{i=1}^{n}$ - реализация выборки из некоторого распределения, т.е. элемент $\mathbb{R}^{\times n}$. Тогда функция
\begin{equation}
	F(x) = \frac{1}{n}\sum_{i=1}^{n}\int_{-\infty}^{x}\delta(x'-X_{i})dx'= \frac{1}{n}\sum_{i=1}^{n}\mathbb{I}(X_{i}\le x)
\end{equation}
называется \textbf{эмпирической функцией распределения} для выборки $\{X_{i}\}_{i=1}^{n}$. Здесь $\delta(x)$ - дельта-функция Дирака, $\mathbb{I}(X_{i}\le x)$ - индикаторная функция луча $[X_{i}, +\infty)$.
\end{definition}
 Аналогичным образом можно определить эмпирическую функцию распределения для выборки как последовательности случайных величин. В таком случае эмпирическая функция распределения сама будет случайной величиной со значением в функциональном пространстве.
\paragraph{Свойства эмпирической функции распределения}
\begin{enumerate}
	\item Монотонность: ЭФР не убывает.
	\item Граничные условия: $\lim_{x\to -\infty}F(x)=0, \lim_{x\to \infty}F(x)=1$.
	\item Непрерывность справа: $\forall x_{0}\in \mathbb{R}: \lim_{x\to_{+} x_{0}}F(x)=F(x_{0})$. При этом функция очевидным образом не является непрерывной ни при каком конечном (и даже счётном) размере выборки.
\end{enumerate}

\begin{theorem}{Теорема Гливенко-Кантелли}{}
	Пусть $\{X_{i}\}_{i=1}^{\infty}$ - последовательность независимых одинаково распределённых $\mathbb{R}$-значных случайных величин с функцией распределения $F(x)$. Пусть $F_{n}(x)$ - эмпирическая функция распределения подпоследовательности $\{X_{i}\}_{i=1}^{n}$. Тогда последовательность случайных величин $$D_{n}=\sup_{x}|F_{n}(x)-F(x)|$$
сходится к 0 почти наверное. 
\end{theorem}

\begin{note}
	Напомним, что из сходимости почти наверное следует сходимость по вероятности.
\end{note}

\begin{definition}{Эмпирическая характеристическая функция}{}
	Эмпирической характеристической функцией выборки $\{X_{i}\}_{i=1}^{n}$ называется функция
\begin{equation}
	\chi_{n}(t):=\frac{1}{n}\sum_{i=1}^{n}e^{itX_{i}}
\end{equation}
\end{definition}

\begin{theorem}{Теорема Гливенко-Кантелли для характеристической функции}{}
	Пусть $\{X_{i}\}_{i=1}^{\infty}$ - последовательность независимых одинаково распределённых случайных величин с характеристической функцией $\chi(t)$. Тогда последовательность $\{\chi_{n}(t)\}_{n=1}^{\infty}$ эмпирических характеристических функций $\{X_{i}\}_{i=1}^{n}$ сходится к $\chi(t)$ почти наверное. 
\end{theorem}

\subsubsection{Задача точечной оценки параметра}
Пусть $\mathcal{P}=\{P_{\theta}, \theta \in \Theta\}$ - семейство распределений. Пусть дана выборка $\{X_{i}\}_{i=1}^{n}$, про которую известно, что распределение, её порождающее, взято из семейства $\mathcal{P}$, но не известно значение $\theta$. Задача оценки параметра сводится к получению информации о значении $\theta$ из $\{X_{i}\}_{i=1}^{n}$. Одним из способов получить информацию о значении $\theta$ является вычисление некоторой \textbf{точечной оценки} для $\theta$, то есть такой статистики, которая при некоторых предположениях и в некотором вероятностном смысле сходится к $\theta$ с ростом $n$, или к некоторой функции этой статистики $\tau(\theta)$.

\begin{definition}{Несмещённая оценка}{}
	Точечная оценка $\hat{\theta}_{n}(\{X\}_{i=1}^{n})$ параметра $\theta$ называется несмещённой, если 
	\begin{equation}
		\forall n: \mathbb{E}[\hat{\theta}_{n}] = \theta
	\end{equation}
\end{definition}

\begin{definition}{Выборочное среднее}{}
	Выборочным средним называется статистика
	\begin{equation}	
	M(\{X_{i}\}_{i=1}^{n}):=\frac{1}{n}\sum_{i=1}^{n}X_{i}
	\end{equation}
\end{definition}

\begin{example}
	Для выборки из любого распределения, имеющего матожидание, выборочное среднее является несмещённой оценкой математического ожидания, что немедленно следует из линейности последнего.
\end{example}


\begin{definition}{Риск}{}
	Риском для выборки $X$ и оценки $\tau$ называется статистика
	\begin{equation}
		R(X):=\mathbb{E}[(\hat{\theta}(X)-\theta)^2]
	\end{equation}
\end{definition}

\begin{theorem}{Риск несмещённой оценки}{}
	Для несмещённой оценки $\hat{\theta}$ её риск равен дисперсии случайной величины, из которой берётся выборка.
\end{theorem}
\begin{proof}
	Пусть $\hat{\theta}$ - несмещённая оценка $\theta$. Тогда
	\begin{align*}
		\E[R(X)]=\E [(\hat{\theta}(X)-\theta)^2] &= \\
		\E [(\hat{\theta}(X) - \E [\hat{\theta}(X)] + \E [\hat{\theta}(X)] - \theta)^2 ] &= \\
		\E[ (\hat{\theta}(X) - \E[\hat{\theta}(X)])^2 + (\E[\hat{\theta}(X)]-\theta)^2 + 2(\hat{\theta}(X)-\E[\hat{\theta}(X)])(\E[\hat{\theta}(X)]-\theta)]  
	\end{align*}
В силу несмещённости $\hat{\theta}$ имеем $\E[\hat{\theta}(X)]=\theta$, следовательно, второе и третье слагаемое в последней строчке зануляются. Остаётся
$$\E[(\hat{\theta}(X)-\E[\hat{\theta}(X)])^2]$$
а это по определению дисперсия $\hat{\theta}(X)$.
\end{proof}

Так называемый \textbf{bias-variance tradeoff} в машинном обучении - это способ интерпретации формулы для матожидания риска
\begin{align*}
	\E[R(X)]=\E[ (\hat{\theta}(X) - \E[\hat{\theta}(X)])^2 +& \\ (\E[\hat{\theta}(X)]-\theta)^2 +& \\ 2(\hat{\theta}(X)-\E[\hat{\theta}(X)])(\E[\hat{\theta}(X)]-\theta)]
\end{align*} 

в которой мы можем изменять характеристики $\hat{\theta}(X)$. Если оценка несмещённая (unbiased), вклад второго и третьего слагаемого, как мы видели выше, нулевой. Путём добавления в оценку смещения (bias) можно попытаться уменьшить дисперсию (variance), но в таком случае станут ненулевыми члены, зависящие от смещения.

\subsubsection{Эффективные оценки}
Пусть выбран некоторый класс оценок $C=\{\hat{\theta}_{\nu}(X)\}_{\nu \in \Lambda}$ (например, несмещённых) параметра $\theta$ для семейства распределений $\mathcal{P}$. При прочих равных мы хотим, чтобы дисперсия нашей оценки была минимальной, поэтому разумно использовать величину дисперсии в качестве критерия качества оценки.
\begin{definition}{Эффективная оценка}{}
	Эффективной оценкой $\hat{\theta}$ в классе $C$ называется такая оценка, что для всякой другой оценки $\hat{\theta}'$ из класса $C$ и для всякого размера $n$ выборки $X$
	$$\E[R(\hat{\theta}(X))]\le \E[R(\hat{\theta}'(X))]$$
\end{definition}    


\begin{definition}{Информация Фишера}{}
	Пусть $\mathcal{P}=\{p_{\theta}\}_{\theta \in \mathbb{R}}$ - однопараметрическое семейство регулярных распределений. Информацией Фишера, содержащейся в выборке $X$ из распределения из этого семейства, называется функция
	\begin{equation}
		\mathcal{I}_{X}(\theta) := \E\left[ \left(\frac{\partial}{\partial \theta}\ln p_{\theta}(X)\right)^2 \right] = \int_{\mathbb{R}} \left(\frac{\partial}{\partial \theta} \ln p_{\theta}(X) \right)^2 p_{\theta}(X)dX
	\end{equation}
\end{definition}




\begin{theorem}{Неравенство Рао-Крамера}{}
	Пусть $\hat{\theta}(X)$ - несмещённая оценка функции $\tau(\theta)$ параметра $\theta$ семейства регулярных распределений $\mathcal{P}=\{p_{\theta}\}$ со значениями в $\mathbb{R}$, и, в частности, $\hat{\theta}(x)$ - эта оценка для случая выборки из одного элемента. Пусть $\theta \in \mathbb{R}$. Пусть конечны для всякого размера выборки $\E[\hat{\theta}(X)]$ и $\mathbb{D}[\hat{\theta}(X)]$. Пусть также выполнены следующие \textit{условия регулярности}:
	\begin{enumerate}
		\item Носитель распределения $p_{\theta}(x)\in \mathcal{P}$ не зависит от $\theta$. 
		\item $\forall \theta \in \mathbb{R}: \forall X \in \text{supp}_{x}(p_{\theta})^{\times n}:\exists \frac{\partial}{\partial \theta}\ln p_{\theta}(X):=\lambda(X, \theta)$
		\item $\frac{\partial}{\partial \theta}\int p_{\theta}(X)dX=\int \frac{\partial}{\partial \theta}p_{\theta}(X)dX = 0$
		\item $\frac{\partial}{\partial \theta}\int \hat{\theta}(X)p_{\theta}(X)dX=\int \hat{\theta}(X)\frac{\partial}{\partial \theta}p_{\theta}(X)dX$
		\item $0 < \mathcal{I}(\theta) < \infty$
	\end{enumerate}
	В таком случае выполняется следующее неравенство:
	\begin{equation}
		\mathbb{D}[\hat{\theta}(X)] \ge \frac{\left(\frac{\partial}{\partial \theta} \tau(\theta)  \right)^2}{\mathcal{I}(\theta)}
	\end{equation}
\end{theorem}

\begin{note}
	Если $\tau(\theta)=\theta$, то 
	\begin{equation}
		\mathbb{D}[\hat{\theta}(X)]\ge \frac{1}{\mathcal{I}(\theta)}
	\end{equation}
\end{note}

\begin{note}
	$$\IFish_{X}{(\theta)}=\E[\lambda^2(X, \theta)]$$
\end{note}

\begin{proof}
	Из условия регулярности 3. имеем
$$0=\int_{\mathbb{R}}\frac{\partial}{\partial \theta}p_{\theta}(X)dX=\int_{\mathbb{R}}\left(\frac{\partial}{\partial\theta}\ln p_{\theta}(X)\right)p_{\theta}(X)dX=\mathbb{E}[\lambda(X, \theta)]$$
где $\lambda(X, \theta)$ - логарифмическая производная плотности $p_{\theta}(X)$ в точке $X$,
$$\lambda(X, \theta)=\pd{}{\theta}\ln p(X, \theta)$$
Из условия регулярности 4 имеем
$$\frac{\partial \tau(\theta)}{\partial\theta}=\int_{\mathbb{R}}\hat{\theta}(X)\left( \frac{\partial}{\partial\theta} \ln p_{\theta}(X) \right)p_{\theta}(X)dx=\E[\hat{\theta}(X)\lambda(X, \theta)]$$
Вычтем из последнего выражения 0, представив его как
$$
\E[\tau(\theta)\lambda(X, \theta)]=\tau(\theta)\E[\lambda(X, \theta)] = 0
$$
используя условие регулярности 3 и тот факт, что $\tau(\theta)$ - не случайная величина. Получим
$$\E[\hat{\theta}(X)\lambda(X, \theta)]-\E[\tau(\theta)\lambda(X, \theta)]=\E[(\hat{\theta}(X)-\tau(\theta))\lambda(X, \theta)]$$
Таким образом,
$$\pd{\tau(\theta)}{\theta}=\E[(\hat{\theta}(X)-\tau(\theta))\lambda(X, \theta)]$$
Используем неравенство Коши-Буняковского-Шварца. Напомним, оно заключается в том, что для любого евклидова пространства, т.е. линейного пространства с невырожденной положительно определённой симметричной билинейной формой (скалярным произведением) $\langle \cdot , \cdot \rangle:V\times V\to \mathbb{R}$ выполняется 
$$\forall a, b\in V: \left| \langle a, b \rangle   \right| \le \sqrt{\langle a, a \rangle \langle b, b\rangle}$$
Случайные величины на одном и том же вероятностном пространстве образуют линейное пространство, в котором роль скалярного произведения играет функционал $X, Y\to \E[XY]$. Фактор-пространство этого пространства по подпространству постоянных случайных величин имеет в качестве скалярного произведения, унаследованного из исходного, ковариацию. В данном случае мы используем исходное скалярное произведение, из которого получаем
$$\E[XY]^2 \le \E[X^2]\E[Y^2]$$
Применяя его к $\frac{\partial \tau(\theta)}{\partial \theta}$, получаем

$$\left( \pd{\tau(\theta)}{\theta} \right)^2 = \left( \E[(\hat{\theta}(X)-\tau(\theta))\lambda(X, \theta)] \right)^{2}\le$$
$$\le \E[(\hat{\theta}(X)-\tau(\theta))^2]\E[\lambda(X, \theta)^2] $$
Поскольку оценка $\hat{\theta}(X)$ несмещённая,
$$\E[(\hat{\theta}(X)-\tau(\theta))^2]=\D[\hat{\theta}(X)]$$
В то же время
$$\E[\lambda(X, \theta)^2]=\cat{I}(\theta)$$
Таким образом,
$$\D[\hat{\theta}(X)]\ge \frac{\left(\pd{\tau(\theta)}{\theta}\right)^2}{\cat{I}_{X}(\theta)}$$
что и требовалось.
\end{proof}

\begin{definition}{Оптимальная оценка}{}
	Оценка $\hat{\theta}(X)$ называется \textbf{оптимальной}, если для неё в неравенстве Рао-Крамера достигается равенство.
\end{definition}

\begin{definition}{Информация Фишера в выборке из одного распределения}{}
$\IFish_{1}(\theta)$ обозначается информация, содержащаяся в выборке из данного распределения из одного элемента с параметром $\theta$.
\end{definition}

\begin{theorem}{Аддитивность информации Фишера}{}
	Если выборка состоит из независимых наблюдений, информация Фишера во всей выборке равна сумме информаций Фишера в каждом из её элементов.
\end{theorem}

Из аддитивности информации Фишера следует, что для выборки размера $n$
$$
\D[\hat{\theta}(X)] \ge \frac{ (\partial \tau(\theta) / \partial \theta )^{2} }{n\cat{I}_{1}(\theta)} \propto \frac{1}{n}
$$
Таким образом, нижняя граница дисперсии обратно пропорциональна размеру выборки, а для оптимальной оценки это также и верхняя граница.

\subsubsection{Экспоненциальное семейство распределений}
Неравенство КБШ, которое мы использовали в доказательстве неравенства Рао-Крамера, обращается в равенство только при условии пропорциональности входящих в него векторов. Векторы, которые мы подставляли в неравенство - $\lambda(X, \theta)$ и $\hat{\theta}(X)-\tau(\theta)$. Предположим, эти векторы пропорциональны с коэффициентом пропорциональности $a$, возможно, зависящим от $\theta$.\

Тогда 
$$
\lambda(X, \theta) = \frac{\partial}{\partial \theta}\ln p_{\theta}(X)=\frac{\hat{\theta}(X)}{a(\theta)}
 - \frac{\tau(\theta)}{a(\theta)} 
$$

Проинтегрируем вторую и третью части равенства по $\theta$. Получим 

\begin{equation}
	\ln p_{\theta}(X) = c(\theta)\hat{\theta}(X) + d(\theta) + S(X)
\end{equation}
где 
$$c(\theta)=\int \frac{1}{a(\theta)}d\theta \ \ \ \ d(\theta) = - \int \frac{\tau(\theta)}{a(\theta)}d\theta$$
а $S(X)$ возникает из константы интегрирования. Экспоненцируя, получаем

\begin{definition}{Экспоненциальное семейство распределений}{}
	Распределения с плотностью вида 
	$$p_{\theta}(X)=\exp\left(c(\theta)\hat{\theta}(X)+ d(\theta) + S(X)  \right)$$
называются распределениями из экспоненциального семейства. 
\end{definition}

\begin{theorem}{Оптимальная статистика для экспоненциального семейства}{}
	Статистика $\hat{\theta}(X)$, участвующая в определении распределения из экспоненциального семейства, является оптимальной для соответствующего распределения.
\end{theorem}

\subsection{Лекция 2}



\section{Тестирование гипотез}



\end{document}
